{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Banana Run\n",
    "\n",
    "This is my solution to the bananas problem for the Udacity [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893). Udacity adapted this problem and runtime environment from the Unity's ML-Agents [Banana Collector environment](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Examples.md#banana-collector). This class runs from May 1, 2019, to August 24, 2019, and this is the first of 5 projects, due June 18th. I started this on May 14th."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this Notebook is educational; to reinforce and deepen ;) my understanding of value-based methods for Deep Reinforcement Learning (DRL). I will attempt to demonstrate a useful skill, in my ability to quickly construct a good solution to a problem which is a stand-in for a large class of real-world problems. I will also attempt to develop an academic comprehension of DNN value-based methods for reinforcement learning by analysing their performance characteristics and comparing them to other forms of machine learning. It is that kind of understanding that may lead me to invent and evaluate novel solutions to similar problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Bananas](banana.gif \"Bananas!\")\n",
    "\n",
    "The problem domain is a virtual world built in the Unity envionment in which yellow and blue bananas fall (apparently at random intervals) from the sky. The agent's objective is to collect as many yellow bananas and as few blue bananas as posible. More precisely, the agent is rewarded +1 point for collecting a yellow banana and rewarded -1 point for collecting blue bananas, and the objective is to get as many points as possible during a fixed-time episode. The agent is considered successful if it is able to average a reward of at least +13 over the course of 100 consecutive episode.\n",
    "\n",
    "Our agent's inputs are 37 dimentional, and include the agent's velocity, along with ray-based perception of objects around the agent's forward direction. Given this information, the agent has to learn how to best select actions. Four discrete actions are available, corresponding to:\n",
    "\n",
    "1. move forward.\n",
    "2. move backward.\n",
    "3. turn left.\n",
    "4. turn right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"../Banana_Linux/Banana.x86_64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Number of actions: 4\n",
      "States look like: [1.         0.         0.         0.         0.84408134 0.\n",
      " 0.         1.         0.         0.0748472  0.         1.\n",
      " 0.         0.         0.25755    1.         0.         0.\n",
      " 0.         0.74177343 0.         1.         0.         0.\n",
      " 0.25854847 0.         0.         1.         0.         0.09355672\n",
      " 0.         1.         0.         0.         0.31969345 0.\n",
      " 0.        ]\n",
      "States have length: 37\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents in the environment\n",
    "print('Number of agents:', len(env_info.agents))\n",
    "\n",
    "# number of actions\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Number of actions:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "state = env_info.vector_observations[0]\n",
    "print('States look like:', state)\n",
    "state_size = len(state)\n",
    "print('States have length:', state_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device = cuda:0\n"
     ]
    }
   ],
   "source": [
    "%aimport dqn_agent\n",
    "%aimport model\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_datum = namedtuple('episode_datum', field_names=['score'])\n",
    "log = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-7-4387ee7168cb>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-4387ee7168cb>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    return x[np.convolve([l.score for l in log], np.ones((10,))/10., mode=\"valid\")\u001b[0m\n\u001b[0m                                                                                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "def running_mean(x, N=100):\n",
    "    return x[np.convolve([l.score for l in log], np.ones((10,))/10., mode=\"valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = dqn_agent.Agent(state_size, action_size, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2499: score: 13.000000 running-mean: 10.47\n"
     ]
    }
   ],
   "source": [
    "episodes = 2500\n",
    "total_score = 0\n",
    "eps = 1.0\n",
    "for e in range(episodes):\n",
    "    # reset the environment\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    state = env_info.vector_observations[0]\n",
    "\n",
    "    eps = max(0.001, eps*.995)\n",
    "    score = 0.\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.act(state, eps)                      # select an action\n",
    "        env_info = env.step(action)[brain_name]             # send the action to the environment\n",
    "        next_state = env_info.vector_observations[0]        # get the next state\n",
    "        reward = env_info.rewards[0]                        # get the reward\n",
    "        done = env_info.local_done[0]                       # see if episode has finished\n",
    "        agent.step(state, action, reward, next_state, done) # maybe learn something\n",
    "        score += reward                                     # update the score\n",
    "        state = next_state                                  # roll over the state to next time step\n",
    "    \n",
    "    log.append(episode_datum(score))\n",
    "    windowed_mean = sum([l.score for l in log[-100:]])/100.0\n",
    "    print('{:d}: score: {:2.0f} running-mean: {:4.2f}'.format(e, score, windowed_mean), end=\"\\r\")\n",
    "    if windowed_mean >= 13:\n",
    "        break\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(log, open('dqn1.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
